# Phase#1 ê°œì„  ì‚¬í•­ ì œì•ˆ

## ì‘ì„±ì¼: 2024-11-20

## í˜„ì¬ ë¬¸ì œì 

### 1. ë³´ìƒ ì²´ê³„ ë¬¸ì œ
```python
# í˜„ì¬
CE detected: +1000
No CE: +1
```

**ë¬¸ì œì :**
- CEê°€ ë§¤ìš° ë“œë¬¼ê²Œ ë°œìƒí•œë‹¤ë©´ agentê°€ ê±°ì˜ í•­ìƒ +1ë§Œ ë°›ìŒ
- 1000:1 ë¹„ìœ¨ì€ ë„ˆë¬´ ê·¹ë‹¨ì 
- íƒìƒ‰(exploration) ìœ ë„ ë©”ì»¤ë‹ˆì¦˜ ë¶€ì¡±
- CE ë°œê²¬ ì „ê¹Œì§€ ì˜ë¯¸ìˆëŠ” í•™ìŠµ ì‹ í˜¸ ì—†ìŒ

### 2. Single Action ì‹¤í–‰ ì„±ëŠ¥ ë¬¸ì œ
- ë§¤ stepë§ˆë‹¤ REST API 1íšŒ í˜¸ì¶œ
- ë„¤íŠ¸ì›Œí¬ latency ëˆ„ì 
- Memory Agentë„ ë§¤ë²ˆ umxc í˜¸ì¶œ (ë§¤ìš° ëŠë¦¼)
- 1536 actionsì„ ëª¨ë‘ ì‹œë„í•˜ë ¤ë©´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼

### 3. íƒìƒ‰ ë¶€ì¡±
- Random explorationë§Œìœ¼ë¡œëŠ” CE ë°œê²¬ ì–´ë ¤ì›€
- íŒ¨í„´ ê°„ ìƒê´€ê´€ê³„ í•™ìŠµ ì–´ë ¤ì›€
- Action spaceê°€ ë„ˆë¬´ í¼ (1536 actions)

---

## ê°œì„  ë°©ì•ˆ

### ğŸ’¡ ì œì•ˆ 1: Shaped Reward + Curiosity-driven Exploration

#### ëª©í‘œ
- CE ë°œê²¬ ì „ì—ë„ ì˜ë¯¸ìˆëŠ” í•™ìŠµ ì‹ í˜¸ ì œê³µ
- íƒìƒ‰ ì¥ë ¤
- íŒ¨í„´ ë‹¤ì–‘ì„± ìœ ë„

#### êµ¬í˜„ ë°©ì•ˆ

```python
def _calculate_reward(self, result: Dict, step_num: int, history: List) -> float:
    """
    ê°œì„ ëœ ë³´ìƒ í•¨ìˆ˜

    1. CE Detection: +100 (ê¸°ì¡´ 1000ì—ì„œ ì¤„ì„)
    2. Progressive reward: ì´ì „ stepê³¼ ë¹„êµ
    3. Pattern diversity bonus: ìƒˆë¡œìš´ íŒ¨í„´ ì‹œë„ ì‹œ
    4. Sequence coherence: ë…¼ë¦¬ì  ì‹œí€€ìŠ¤ ë³´ë„ˆìŠ¤
    """
    reward = 0.0

    if result['ce_detected']:
        # ===== CE ë°œê²¬ =====
        # í° ë³´ìƒ
        reward += 100.0

        # CE ê°œìˆ˜ì— ë¹„ë¡€ (ë§ì´ ë°œê²¬í• ìˆ˜ë¡ ì¢‹ìŒ)
        reward += result['ce_total'] * 5.0

        # ì§§ì€ ì‹œí€€ìŠ¤ ë³´ë„ˆìŠ¤ (ë¹¨ë¦¬ ì°¾ì„ìˆ˜ë¡ ì¢‹ìŒ)
        reward += 50.0 / step_num

    else:
        # ===== CE ì—†ìŒ - í•˜ì§€ë§Œ ìœ ìš©í•œ ì •ë³´ =====

        # 1. Novelty bonus: ìƒˆë¡œìš´ action ì‹œë„
        if action not in history:
            reward += 5.0  # íƒìƒ‰ ì¥ë ¤
        else:
            reward -= 1.0  # ì¤‘ë³µ íŒ¨ë„í‹°

        # 2. Pattern exploration bonus
        operation_type = action // 256
        pattern = action % 256

        # ë‹¤ì–‘í•œ operation ì‹œë„ ì¥ë ¤
        unique_ops = len(set([a // 256 for a in history]))
        reward += unique_ops * 0.5

        # ë‹¤ì–‘í•œ pattern ì‹œë„ ì¥ë ¤
        unique_patterns = len(set([a % 256 for a in history]))
        reward += unique_patterns * 0.1

        # 3. íŠ¹ì • íŒ¨í„´ ì¡°í•© ë³´ë„ˆìŠ¤ (heuristic)
        if step_num > 1:
            prev_op = history[-1] // 256
            curr_op = operation_type

            # March-like sequence ë³´ë„ˆìŠ¤
            if prev_op == 0 and curr_op == 1:  # ASCâ†’DESC
                reward += 2.0
            elif prev_op == 1 and curr_op == 0:  # DESCâ†’ASC
                reward += 2.0

            # Cross pattern ë³´ë„ˆìŠ¤
            if prev_op in [0, 1] and curr_op in [2, 3]:
                reward += 1.5

    # 4. Time penalty (ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ í˜ë„í‹°)
    reward -= step_num * 0.1

    return reward
```

#### ìˆ˜ì • íŒŒì¼
- `src/RLAgent/phase1_environment_distributed.py`
  - `_calculate_reward()` í•¨ìˆ˜ êµì²´
  - `step()` í•¨ìˆ˜ì—ì„œ history ì „ë‹¬

#### ì˜ˆìƒ íš¨ê³¼
- CE ì—†ì–´ë„ íƒìƒ‰ ìœ ë„
- ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
- í•™ìŠµ ì†ë„ í–¥ìƒ

---

### ğŸ’¡ ì œì•ˆ 2: Batch Executionìœ¼ë¡œ ì„±ëŠ¥ ê°œì„  â­â­â­ (ìµœìš°ì„ )

#### ëª©í‘œ
- REST API í˜¸ì¶œ íšŸìˆ˜ ëŒ€í­ ê°ì†Œ
- umxc í˜¸ì¶œ íšŸìˆ˜ ê°ì†Œ
- ì‹¤í–‰ ì‹œê°„ 10ë°° ì´ìƒ ë‹¨ì¶•

#### A. Environment ìˆ˜ì •

```python
class Phase1EnvironmentDistributed(gym.Env):
    def __init__(
        self,
        memory_agent_url: str = "http://192.168.3.20:5000",
        batch_size: int = 5,  # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°!
        ...
    ):
        self.batch_size = batch_size
        self.action_buffer = []

    def step(self, action: int):
        """Buffered step execution"""
        self.action_buffer.append(action)
        self.step_count += 1

        # Bufferê°€ ì°¼ê±°ë‚˜ ë§ˆì§€ë§‰ stepì´ë©´ batch ì‹¤í–‰
        should_execute = (
            len(self.action_buffer) >= self.batch_size or
            self.step_count >= self.max_seq_len
        )

        if should_execute:
            # Batch ì‹¤í–‰
            results = self._execute_batch_remote(self.action_buffer)

            # CE ë°œê²¬ ì—¬ë¶€ í™•ì¸
            ce_detected_at = None
            for i, result in enumerate(results):
                if result['ce_detected']:
                    ce_detected_at = i
                    break

            # Buffer ì´ˆê¸°í™”
            self.action_buffer = []

            # ê²°ê³¼ ì²˜ë¦¬
            if ce_detected_at is not None:
                # CE ë°œê²¬!
                final_result = results[ce_detected_at]
                terminated = True
                reward = self._calculate_reward(final_result, self.step_count)
            else:
                # ëª¨ë‘ PASS
                final_result = results[-1]
                terminated = False
                reward = sum([self._calculate_reward(r, i+1)
                             for i, r in enumerate(results)])

            return self._get_observation(), reward, terminated, False, {}
        else:
            # Buffer ì¤‘ê°„ - ì•„ë¬´ê²ƒë„ ë°˜í™˜ ì•ˆí•¨ (gym.Env í™•ì¥ í•„ìš”)
            # ë˜ëŠ” intermediate reward ë°˜í™˜
            return self._get_observation(), 0.0, False, False, {}

    def _execute_batch_remote(self, actions: List[int]) -> List[Dict]:
        """
        Batchë¡œ actions ì‹¤í–‰

        Args:
            actions: List of action indices

        Returns:
            List of results (CE ë°œê²¬ ì‹œ ì¤‘ê°„ì— ì¤‘ë‹¨ë¨)
        """
        response = requests.post(
            f"{self.memory_agent_url}/execute_batch",
            json={'actions': actions},
            timeout=self.timeout * len(actions)
        )
        response.raise_for_status()
        return response.json()['results']
```

#### B. Memory Agent Server API ì¶”ê°€

```python
# src/MemoryAgent/memory_agent_server.py

@app.route('/execute_batch', methods=['POST'])
def execute_batch():
    """
    ë°°ì¹˜ ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸

    Request: {
        "actions": [0, 1, 2, 3, 4]  # ì—¬ëŸ¬ action
    }

    Response: {
        "results": [
            {"ce_detected": False, "ce_total": 0, ...},
            {"ce_detected": False, "ce_total": 0, ...},
            {"ce_detected": True, "ce_total": 5, ...},  # ì—¬ê¸°ì„œ ì¤‘ë‹¨!
        ],
        "stopped_at": 2,  # CE ë°œê²¬í•œ index
        "total_executed": 3
    }
    """
    try:
        data = request.get_json()
        actions = data.get('actions', [])

        if not actions:
            return jsonify({'error': 'No actions provided'}), 400

        results = []

        for i, action in enumerate(actions):
            logging.info(f"Batch action {i}/{len(actions)}: {action}")

            # Execute via C library
            ce_info, success = memory_agent.execute_action(action)

            # Decode action
            operation_type, pattern = memory_agent.decode_action(action)

            result = {
                'success': success,
                'ce_detected': ce_info.has_errors(),
                'ce_volatile': ce_info.volatile_count,
                'ce_persistent': ce_info.persistent_count,
                'ce_total': ce_info.total_count,
                'temperature': ce_info.temperature,
                'operation': OperationType.name(operation_type),
                'pattern': f'0x{pattern:02X}'
            }
            results.append(result)

            # CE ë°œê²¬í•˜ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
            if ce_info.has_errors():
                logging.info(f"CE detected at batch index {i}, stopping")
                return jsonify({
                    'results': results,
                    'stopped_at': i,
                    'total_executed': i + 1,
                    'ce_detected': True
                })

        # ëª¨ë‘ PASS
        return jsonify({
            'results': results,
            'stopped_at': len(actions) - 1,
            'total_executed': len(actions),
            'ce_detected': False
        })

    except Exception as e:
        logging.error(f"Batch execution error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500
```

#### C. C Library ìµœì í™” (ì„ íƒì  - ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒ)

```c
// include/memory_agent.h
int ma_execute_batch(int* actions, int count, ActionResult* results);

// src/MemoryAgent/c_library/memory_agent.c

int ma_execute_batch(int* actions, int count, ActionResult* results) {
    // Baseline í•œ ë²ˆë§Œ ê¸°ë¡
    CEInfo baseline;
    execute_umxc(&baseline);

    // ëª¨ë“  actions ì‹¤í–‰
    for (int i = 0; i < count; i++) {
        int operation_type = actions[i] / 256;
        unsigned char pattern = actions[i] % 256;

        // Execute operation
        switch (operation_type) {
            case WR_ASC_ASC:
                write_ascending(pattern);
                read_ascending(pattern);
                break;
            case WR_DESC_DESC:
                write_descending(pattern);
                read_descending(pattern);
                break;
            // ... other operations
        }

        // ì¤‘ê°„ CE ì²´í¬ (ì„ íƒì )
        if (i % 5 == 0) {  // 5ê°œë§ˆë‹¤ ì²´í¬
            CEInfo current;
            execute_umxc(&current);

            if (current.total_count > baseline.total_count) {
                // CE ë°œê²¬! ì¤‘ë‹¨
                results[i].ce_info = current;
                return i + 1;  // ì‹¤í–‰í•œ ê°œìˆ˜ ë°˜í™˜
            }
        }
    }

    // ë§ˆì§€ë§‰ umxc í•œ ë²ˆë§Œ í˜¸ì¶œ
    CEInfo final;
    execute_umxc(&final);

    // Delta ê³„ì‚°
    results[count-1].ce_info.total_count =
        final.total_count - baseline.total_count;

    return count;  // ëª¨ë‘ ì‹¤í–‰
}
```

#### ìˆ˜ì • íŒŒì¼
1. `src/RLAgent/phase1_environment_distributed.py`
   - `__init__()`: batch_size íŒŒë¼ë¯¸í„° ì¶”ê°€
   - `step()`: buffering ë¡œì§ ì¶”ê°€
   - `_execute_batch_remote()`: ìƒˆ í•¨ìˆ˜ ì¶”ê°€

2. `src/MemoryAgent/memory_agent_server.py`
   - `/execute_batch` ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

3. (ì„ íƒ) `src/MemoryAgent/c_library/memory_agent.c`
   - `ma_execute_batch()` í•¨ìˆ˜ ì¶”ê°€

4. (ì„ íƒ) `src/MemoryAgent/memory_agent_c_wrapper.py`
   - `execute_batch()` Python wrapper ì¶”ê°€

#### ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 
```
í˜„ì¬: 1 action = 1 REST call + 1 umxc call
     10 actions = 10 REST + 10 umxc â‰ˆ 30ì´ˆ

ê°œì„ : 10 actions = 1 REST call + 1 umxc call â‰ˆ 3ì´ˆ
     â†’ 10ë°° ë¹ ë¦„!

C batchê¹Œì§€ ì ìš©:
     10 actions = 1 REST call + 1 umxc call â‰ˆ 1ì´ˆ
     â†’ 30ë°° ë¹ ë¦„!
```

---

### ğŸ’¡ ì œì•ˆ 3: Curriculum Learning (ë‹¨ê³„ì  ë‚œì´ë„)

#### ëª©í‘œ
- ì´ˆê¸° í•™ìŠµ ì†ë„ í–¥ìƒ
- Action spaceë¥¼ ì ì§„ì ìœ¼ë¡œ í™•ì¥
- ì‰¬ìš´ ë¬¸ì œë¶€í„° ì–´ë ¤ìš´ ë¬¸ì œë¡œ

#### êµ¬í˜„ ë°©ì•ˆ

```python
class CurriculumPhase1Environment(Phase1EnvironmentDistributed):
    """ë‹¨ê³„ì  ë‚œì´ë„ ì¦ê°€"""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.difficulty_level = 0
        self.success_count = 0
        self.total_episodes = 0

    def get_allowed_actions(self):
        """ë‚œì´ë„ì— ë”°ë¼ í—ˆìš©ëœ actions ë°˜í™˜"""
        if self.difficulty_level == 0:
            # Level 0: 2ê°œ operation, 16ê°œ pattern
            # Total: 32 actions
            ops = [0, 1]  # WR_ASC_ASC, WR_DESC_DESC
            patterns = list(range(16))  # 0x00-0x0F

        elif self.difficulty_level == 1:
            # Level 1: 4ê°œ operation, 64ê°œ pattern
            # Total: 256 actions
            ops = [0, 1, 2, 3]
            patterns = list(range(64))

        elif self.difficulty_level == 2:
            # Level 2: 6ê°œ operation, 128ê°œ pattern
            # Total: 768 actions
            ops = list(range(6))
            patterns = list(range(128))

        else:
            # Level 3: ì „ì²´
            # Total: 1536 actions
            ops = list(range(6))
            patterns = list(range(256))

        # Action list ìƒì„±
        actions = []
        for op in ops:
            for pat in patterns:
                actions.append(op * 256 + pat)

        return actions

    def step(self, action: int):
        """Step with curriculum"""
        # Allowed actionsë§Œ í—ˆìš©
        allowed = self.get_allowed_actions()
        if action not in allowed:
            # í—ˆìš© ì•ˆëœ action â†’ ê°€ì¥ ê°€ê¹Œìš´ ê²ƒìœ¼ë¡œ ë§¤í•‘
            action = min(allowed, key=lambda x: abs(x - action))

        obs, reward, terminated, truncated, info = super().step(action)

        # Episode ì¢…ë£Œ ì‹œ í†µê³„ ì—…ë°ì´íŠ¸
        if terminated or truncated:
            self.total_episodes += 1
            if terminated:
                self.success_count += 1

            # ë‚œì´ë„ ì¡°ì •
            self._update_difficulty()

        return obs, reward, terminated, truncated, info

    def _update_difficulty(self):
        """ì„±ê³µë¥ ì— ë”°ë¼ ë‚œì´ë„ ì¦ê°€"""
        if self.total_episodes < 10:
            return  # ìµœì†Œ 10 episodes í•„ìš”

        success_rate = self.success_count / self.total_episodes

        # 70% ì´ìƒ ì„±ê³µí•˜ë©´ ë‚œì´ë„ ì¦ê°€
        if success_rate > 0.7 and self.difficulty_level < 3:
            self.difficulty_level += 1
            logging.info(f"Difficulty increased to level {self.difficulty_level}")
            logging.info(f"New action space: {len(self.get_allowed_actions())} actions")

            # í†µê³„ ì´ˆê¸°í™”
            self.success_count = 0
            self.total_episodes = 0
```

#### ìˆ˜ì • íŒŒì¼
- ìƒˆ íŒŒì¼: `src/RLAgent/phase1_curriculum_environment.py`
- ë˜ëŠ” `phase1_environment_distributed.py`ì— í†µí•©

#### ì˜ˆìƒ íš¨ê³¼
- ì´ˆê¸° í•™ìŠµ 30% ì´ìƒ ë¹ ë¦„
- ìˆ˜ë ´ ì•ˆì •ì„± í–¥ìƒ

---

### ğŸ’¡ ì œì•ˆ 4: Prior Knowledge Injection (ì‚¬ì „ ì§€ì‹ í™œìš©)

#### ëª©í‘œ
- ì•Œë ¤ì§„ ì¢‹ì€ íŒ¨í„´ìœ¼ë¡œ bootstrap
- íƒìƒ‰ ê³µê°„ íš¨ìœ¨ì ìœ¼ë¡œ ì¤„ì„

#### êµ¬í˜„ ë°©ì•ˆ

```python
def get_heuristic_action_sequences():
    """
    ì•Œë ¤ì§„ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ íŒ¨í„´ë“¤

    Returns:
        List of action sequences
    """
    sequences = []

    # 1. Walking 1/0 pattern
    sequences.append([
        0 * 256 + 0x00,  # ASC write all 0
        0 * 256 + 0xFF,  # ASC write all 1
    ])

    # 2. Checkerboard pattern
    sequences.append([
        0 * 256 + 0xAA,  # ASC write 0xAA
        0 * 256 + 0x55,  # ASC write 0x55
    ])

    # 3. March C- like
    sequences.append([
        0 * 256 + 0x00,  # ASC write 0
        0 * 256 + 0xFF,  # ASC read 0, write 1
        1 * 256 + 0x00,  # DESC read 1, write 0
        1 * 256 + 0x00,  # DESC read 0
    ])

    # 4. Row hammer like (cross pattern)
    sequences.append([
        2 * 256 + 0xAA,  # ASC write, DESC read 0xAA
        3 * 256 + 0x55,  # DESC write, ASC read 0x55
    ])

    # 5. Sliding diagonal
    sequences.append([
        4 * 256 + 0xF0,  # DESC single 0xF0
        5 * 256 + 0x0F,  # ASC single 0x0F
    ])

    # 6. Complementary patterns
    for pattern in [0x00, 0xFF, 0xAA, 0x55, 0xF0, 0x0F]:
        sequences.append([
            0 * 256 + pattern,
            0 * 256 + (pattern ^ 0xFF),  # Complement
        ])

    return sequences


# Training scriptì—ì„œ ì‚¬ìš©
def pretrain_with_heuristics(agent, env, sequences):
    """
    Heuristic sequencesë¡œ pre-training

    Args:
        agent: RL agent (DQN or PPO)
        env: Environment
        sequences: List of action sequences
    """
    logging.info("Pre-training with heuristic sequences...")

    for seq_idx, sequence in enumerate(sequences):
        obs, _ = env.reset()

        for step, action in enumerate(sequence):
            obs, reward, terminated, truncated, info = env.step(action)

            # Replay bufferì— ì¶”ê°€
            if hasattr(agent, 'replay_buffer'):
                agent.replay_buffer.add(obs, action, reward, obs, terminated)

            if terminated or truncated:
                break

        logging.info(f"Heuristic sequence {seq_idx+1}/{len(sequences)}: "
                    f"{len(sequence)} steps, final reward={reward}")

    # Pre-training
    if hasattr(agent, 'train_step'):
        for _ in range(100):
            agent.train_step()

    logging.info("Pre-training complete")
```

#### ìˆ˜ì • íŒŒì¼
- ìƒˆ íŒŒì¼: `src/RLAgent/heuristics.py`
- Training scriptì—ì„œ import í›„ ì‚¬ìš©

#### ì˜ˆìƒ íš¨ê³¼
- ì´ˆê¸° íƒìƒ‰ íš¨ìœ¨ í–¥ìƒ
- ë¹ ë¥¸ baseline í™•ë³´

---

## êµ¬í˜„ ìš°ì„ ìˆœìœ„

### ğŸ¥‡ 1ìˆœìœ„: Batch Execution (í•„ìˆ˜)
**ì´ìœ :**
- ì„±ëŠ¥ 10-30ë°° í–¥ìƒ
- êµ¬í˜„ ë¹„êµì  ê°„ë‹¨
- ì¦‰ì‹œ íš¨ê³¼ í¼

**ì‘ì—…ëŸ‰:** ì¤‘ê°„
**ì˜ˆìƒ ì‹œê°„:** 2-3ì‹œê°„

---

### ğŸ¥ˆ 2ìˆœìœ„: Shaped Reward (ì¤‘ìš”)
**ì´ìœ :**
- CE ì—†ì–´ë„ í•™ìŠµ ê°€ëŠ¥
- íƒìƒ‰ í¬ê²Œ ê°œì„ 
- í•™ìŠµ ì†ë„ í–¥ìƒ

**ì‘ì—…ëŸ‰:** ì ìŒ
**ì˜ˆìƒ ì‹œê°„:** 1ì‹œê°„

---

### ğŸ¥‰ 3ìˆœìœ„: Curriculum Learning (ì„ íƒ)
**ì´ìœ :**
- ì´ˆê¸° ìˆ˜ë ´ ë¹ ë¦„
- ì•ˆì •ì„± í–¥ìƒ
- í•˜ì§€ë§Œ í•„ìˆ˜ëŠ” ì•„ë‹˜

**ì‘ì—…ëŸ‰:** ì¤‘ê°„
**ì˜ˆìƒ ì‹œê°„:** 2ì‹œê°„

---

### 4ìˆœìœ„: Prior Knowledge (ë³´ì¡°)
**ì´ìœ :**
- Bootstrap ë„ì›€
- í•˜ì§€ë§Œ ì—†ì–´ë„ ë¨
- Heuristicì— ì˜ì¡´

**ì‘ì—…ëŸ‰:** ì ìŒ
**ì˜ˆìƒ ì‹œê°„:** 30ë¶„

---

## êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: ì„±ëŠ¥ ê°œì„  (Week 1)
- [ ] Batch Execution API ì¶”ê°€
- [ ] Environment batch ì§€ì›
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### Phase 2: í•™ìŠµ ê°œì„  (Week 1-2)
- [ ] Shaped Reward êµ¬í˜„
- [ ] ë³´ìƒ í•¨ìˆ˜ íŠœë‹
- [ ] í•™ìŠµ ì‹¤í—˜

### Phase 3: ê³ ê¸‰ ê¸°ëŠ¥ (Week 2+)
- [ ] Curriculum Learning (ì„ íƒ)
- [ ] Prior Knowledge (ì„ íƒ)
- [ ] ìµœì¢… íŠœë‹

---

## ì°¸ê³ ì‚¬í•­

### Batch Size ì„ íƒ
- **Small (2-3)**: ë¹ ë¥¸ í”¼ë“œë°±, CE ì¡°ê¸° ë°œê²¬
- **Medium (5-10)**: ê· í˜•ì 
- **Large (20+)**: ìµœëŒ€ ì„±ëŠ¥, CE ë°œê²¬ ëŠ¦ì„ ìˆ˜ ìˆìŒ

**ê¶Œì¥:** 5-10 (ê· í˜•)

### Reward Tuning
ì´ˆê¸°ê°’ìœ¼ë¡œ ì‹œì‘ í›„ ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì •:
- CE detected: 100
- Novelty: 5
- Diversity: 0.5
- Time penalty: 0.1

### Monitoring
ë‹¤ìŒ ë©”íŠ¸ë¦­ ì¶”ì :
- Success rate (CE ë°œê²¬ë¥ )
- Average sequence length
- Unique actions tried
- Execution time per episode

---

## ë‹¤ìŒ íšŒì˜ ì‹œ ë…¼ì˜ ì‚¬í•­

1. **Batch size** ì–¼ë§ˆë¡œ ì‹œì‘í• ê¹Œ?
2. **Reward weights** ì´ˆê¸°ê°’ ê²€í† 
3. **Curriculum** í•„ìš”í•œê°€?
4. **Heuristics** ì–´ë–¤ íŒ¨í„´ í¬í•¨í• ê¹Œ?

---

**ì‘ì„±ì**: Claude Code
**ê²€í†  í•„ìš”**: ì‚¬ìš©ì ê²€í†  í›„ êµ¬í˜„ ì‹œì‘
